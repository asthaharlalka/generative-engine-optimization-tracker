# GEO Visibility Tracker

**Find out which brands AI recommends when your buyers ask category questions â€” before your competitors do.**

Built by [Astha Harlalka](https://asthaharlalka.com) Â· [Live Demo â†’](https://geo-tracker.asthaharlalka.com)

---

## What is GEO?

Generative Engine Optimization (GEO) is the emerging discipline of making your brand visible inside AI-generated answers â€” not just search engine results pages.

94% of B2B buyers now use AI tools like ChatGPT, Perplexity, Claude, and Gemini to research vendors and products before making a purchase decision. If your brand isn't appearing in those answers, you're invisible to buyers who have already moved past Google.

This tool measures exactly that: which brands AI recommends when real buyers ask real questions in your category.

---

## What It Does

The GEO Visibility Tracker fires a set of buyer-intent questions at up to 5 AI engines simultaneously, counts how often each brand appears in the responses, and produces a visibility scorecard with an actionable optimization playbook.

**Works for any category** â€” B2B software, consumer products, FMCG, services, retail. The question generation adapts to the type of category you enter.

---

## Features

- **Any category** â€” type anything: "Running Shoes", "GTM Automation Software", "Specialty Coffee Brands", "Skincare for Oily Skin"
- **Up to 5 LLMs simultaneously** â€” ChatGPT, Perplexity, Claude, Gemini, Grok. Run with any combination you have keys for
- **Smart question generation** â€” AI generates 10 brand-intent questions tailored to your category and buying context. Questions are validated to ensure every query gives all brands a fair chance to appear
- **Your brand vs competitors** â€” enter your own brand separately so it always appears first in results
- **Custom questions** â€” add your own on top of the AI-generated set. Invalid questions (head-to-heads, brand-named, non-intent) are flagged with the reason
- **Visibility scorecard** â€” scored 0â€“100 based on breadth (questions appeared in), frequency (total mentions), and sentiment
- **Per-LLM breakdown** â€” see exactly which AI engines are recommending which brands
- **GEO Optimization Playbook** â€” after every scan, select any brand and get a tiered action plan based on their score: what to do, why it works, how to do it, effort and impact rating
- **Session-persistent API keys** â€” keys stay in the sidebar for the duration of your tab session. Never stored, cleared on tab close

---

## How Scoring Works

Each brand is scored 0â€“100:

| Component | Weight | What it measures |
|---|---|---|
| Breadth | 60% | How many of the total questions the brand appeared in |
| Frequency | 25% | Total mention count across all LLMs (capped to prevent inflation) |
| Sentiment | 15% | Positive sentiment detected in context around brand mentions |

**Risk tiers:**
- ðŸŸ¢ Visible â€” appeared in 4+ questions
- ðŸŸ¡ Weak â€” appeared in 1â€“3 questions
- ðŸ”´ Invisible â€” appeared in 0 questions

---

## Question Quality Rules

Every question â€” AI-generated or custom â€” must pass all of these to run:

1. **Brand-intent required** â€” the answer must naturally require naming one or more brands
2. **No brand names in the question** â€” keeps the field open so any brand can win
3. **No head-to-head comparisons** â€” "X vs Y" questions only produce two brand names and distort scoring for everyone else
4. **No generic education questions** â€” "how do I choose a running shoe" produces no brand recommendations
5. **Context-appropriate** â€” consumer questions for consumer categories, B2B questions for software

---

## Tech Stack

- [Streamlit](https://streamlit.io) â€” UI and app framework
- [OpenAI API](https://platform.openai.com) â€” GPT-4o-mini for question generation and scanning
- [Perplexity API](https://perplexity.ai) â€” sonar-small-online model
- [Anthropic API](https://anthropic.com) â€” Claude 3 Haiku
- [Google Gemini API](https://aistudio.google.com) â€” Gemini 1.5 Flash
- [xAI Grok API](https://console.x.ai) â€” Grok Beta
- [Plotly](https://plotly.com) â€” interactive charts
- [Pandas](https://pandas.pydata.org) â€” data processing

---

## Setup

### Prerequisites

- Python 3.9+
- API keys for at least one of the supported LLMs

### Local Installation

```bash
git clone https://github.com/asthaharlalka/geo-tracker
cd geo-tracker
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
streamlit run app.py
```

App opens at `http://localhost:8501`.

### API Keys

Get keys from:
- **OpenAI** â†’ [platform.openai.com](https://platform.openai.com)
- **Perplexity** â†’ [perplexity.ai/settings/api](https://perplexity.ai/settings/api)
- **Anthropic** â†’ [console.anthropic.com](https://console.anthropic.com)
- **Gemini** â†’ [aistudio.google.com](https://aistudio.google.com)
- **Grok** â†’ [console.x.ai](https://console.x.ai)

Keys are entered in the sidebar each session and are never stored or logged.

---

## Deployment

### Streamlit Cloud (recommended)

1. Push this repo to GitHub
2. Go to [share.streamlit.io](https://share.streamlit.io) and connect your repo
3. Set main file as `app.py`, deploy
4. Add custom domain in Streamlit app settings
5. Add CNAME record on your domain pointing to your Streamlit app URL

---

## Cost Per Scan

A typical 10-question scan across all 5 LLMs costs approximately $0.02â€“0.05 depending on response length. GPT-4o-mini is the most cost-efficient. Perplexity charges per request. Claude Haiku and Gemini Flash are both very low cost.

---

## What GEO Optimization Actually Looks Like

After seeing your score, the app gives you a tiered playbook. The underlying principles:

**LLMs recommend brands that:**
- Have structured content directly answering buyer questions (FAQ pages, comparison pages, use-case pages)
- Are cited across authoritative third-party sources (G2, Reddit, Capterra, industry blogs, press)
- Use language that matches how buyers actually phrase questions
- Appear in training data through reviews, case studies, and analyst mentions

**The biggest levers by score tier:**
- Score 0 â†’ Get on G2, build comparison pages, answer Reddit questions, get press mentions
- Score 1â€“30 â†’ Deepen use-case content, build "vs competitor" pages, increase review recency
- Score 31â€“60 â†’ Fill specific question gaps, get cited by LLM-visible sources, publish original research
- Score 60+ â†’ Defend position, expand to adjacent categories, use your score as a marketing asset

---

## License

MIT â€” use it, fork it, build on it.
